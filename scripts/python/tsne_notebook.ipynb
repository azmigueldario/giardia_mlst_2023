{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE projection of Giardia spp. distance matrix\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "**Aim:**\n",
    "Subsample a set of Giardia genomes to reduce overrepresentation of highly similar assemblies in a gene-by-gene schema\n",
    "\n",
    "\n",
    "Project a distance matrix obtained using **mash** (kmer size = 51) to a vector space and then use clustering to determine the similarity among them. Finally, to obtain a curated set of genomes, we will conduct a grouped subsample.\n",
    "\n",
    "**Data input:**  Import a square matrix of pre-calculated mash distances (*sourmash v4.8.9*)\n",
    "\n",
    "The running environment requirements for this notebook are contained in the same folder. By running `conda install --file environment.yml` you will have a conda environment with the necessary dependencies and the kernel for an interactive notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Import all dependencies \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as colormaps\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import HDBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # load the curated samples (meeting HQ criteria)   \n",
    "sour_data = pd.read_csv('../../output/sourmash_HQ_dist.csv', sep=',')\n",
    "\n",
    "    # sanitize the labels\n",
    "sour_data.columns = sour_data.columns.str.replace('.*/', '', regex=True)\n",
    "sour_data.index = sour_data.columns\n",
    "\n",
    "    # random seed for reproducibility\n",
    "seed = 112233\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "This loop will show us different topologies according to the perplexity value (influence of neighbors in clustering). The metric is set to `precomputed` as it comes directly from a MASH square distance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of perplexity and iterations\n",
    "perplexity = np.arange(20, 88, 15)\n",
    "iters =  (250, 500, 1000, 2500, 5000)\n",
    "\n",
    "\n",
    "# produce a multiplot of results by perplexity in the same max_iter value\n",
    "# capture divergence value for each iteration\n",
    "\n",
    "for j in iters:\n",
    "    \n",
    "    divergence = []        \n",
    "    fig, axs = plt.subplots(3,2, figsize=(12,15), facecolor='w', layout=\"constrained\")\n",
    "    fig.suptitle(\"Topology at different perplexity (p) values with \" +\n",
    "                 str(j) + \" iterations\",  \n",
    "                fontsize=16, va='bottom', weight='bold') \n",
    "    axs = axs.ravel()\n",
    "\n",
    "        # pair fit [KL divergence] with perplexity values\n",
    "    for index, p in enumerate(perplexity):\n",
    "        \n",
    "        model = TSNE(\n",
    "            n_components=2, \n",
    "            metric='precomputed', init=\"random\",\n",
    "            perplexity=p, max_iter=j)\n",
    "        fitted = model.fit_transform(sour_data)\n",
    "        divergence.append(model.kl_divergence_)\n",
    "        \n",
    "            # plot 2D projections\n",
    "        axs[index].scatter(fitted[:, 0], fitted[:,1], \n",
    "                        color=plt.cm.tab20(index) )\n",
    "        axs[index].set_xlabel(\"p = \" + str(p),\n",
    "                            fontsize=14)\n",
    "        \n",
    "        # plot perplexity vs KL divergence \n",
    "    plt.plot(\n",
    "        perplexity, divergence, color='red', \n",
    "        marker='o', fillstyle='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find convergence of clustering with at least 1000 iterations using a perplexity value above ~50 when only analyzing the HQ samples. We run this model a few times too to make sure that the same topology is reproduced.\n",
    "\n",
    "A fitted model embeddings with a random seed (for reproducibility) is saved. Proper labels are attached to this new data frame based on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(3,3, figsize=(15,15), \n",
    "                facecolor='w', layout=\"constrained\")\n",
    "fig.suptitle(\"Examples of iterations with selected hyperparameters\",\n",
    "        fontsize=16, va='bottom', weight='bold') \n",
    "axs = axs.ravel()\n",
    "\n",
    "for index in range(0,9):\n",
    "        model = TSNE(n_components=2, metric=\"precomputed\",\n",
    "                     init=\"random\", perplexity=65,\n",
    "                     max_iter=2500)\n",
    "        fitted = model.fit_transform(sour_data)\n",
    "        \n",
    "        axs[index].scatter(fitted[:, 0], fitted[:, 1],\n",
    "                           color=plt.cm.tab20(index))\n",
    "\n",
    "        ## saving embeddings with a seed number\n",
    "tsne = TSNE(\n",
    "        n_components=2, random_state=seed, \n",
    "        metric='precomputed', init=\"random\",\n",
    "        perplexity=65, max_iter=2500)\n",
    "tsne = tsne.fit_transform(sour_data)\n",
    "\n",
    "vector_mat = pd.DataFrame(tsne, columns=['t-SNE-1', 't-SNE-2'])\n",
    "vector_mat.index= sour_data.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a ploting function\n",
    "\n",
    "Imported and modified the code for a plot of the HDBSCAN results. It takes a numpy array with the clustering labels to produce a scatter plot colored by cluster [if available]. Each point's size represents the probability of belonging to its cluster.\n",
    "\n",
    "- Unclustered values are marked with a black \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X, labels, probabilities=None, parameters=None, ground_truth=False, ax=None):\n",
    "        # transforms if inputs are pd.dataframe so the np operations can be performed\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values  \n",
    "    if isinstance(labels, pd.Series):\n",
    "        labels = labels.values\n",
    "        # creates a new ploting space is axes is not specified\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(10, 7))\n",
    "        # labels and probabilities are set to \"1\" if not specified\n",
    "    labels = labels if labels is not None else np.ones(X.shape[0])\n",
    "    probabilities = probabilities if probabilities is not None else np.ones(X.shape[0])\n",
    "        # selects colors in spectra palette according to set number\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "        # The probability of a point belonging to its labeled cluster determines\n",
    "        # the size of its marker\n",
    "    proba_map = {idx: probabilities[idx] for idx in range(len(labels))}    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "        class_index = np.where(labels == k)[0]\n",
    "        for ci in class_index:\n",
    "            ax.plot(\n",
    "                X[ci, 0],\n",
    "                X[ci, 1],\n",
    "                \"x\" if k == -1 else \"o\",\n",
    "                markerfacecolor=tuple(col),\n",
    "                markeredgecolor=\"k\",\n",
    "                markersize=6 if k == -1 else 5 + 7 * proba_map[ci],\n",
    "            )\n",
    "        # Improve labelling\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    preamble = \"True\" if ground_truth else \"Estimated\"\n",
    "    title = f\"{preamble} number of clusters: {n_clusters_}\"\n",
    "    if parameters is not None:\n",
    "        parameters_str = \", \".join(f\"{k}={v}\" for k, v in parameters.items())\n",
    "        title += f\" \\n {parameters_str}\"\n",
    "    ax.set_title(title,  fontsize=14)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning\n",
    "\n",
    "HDBSCAN will find the optimal epsilon automatically, but we should tune the `min_cluster_size` and the `min_samples` values. \n",
    "\n",
    "- We see that small `min_cluster_sizes` (~ 3) are not ideal as they create many small clusters with doubtful separation\n",
    "- If we go above 30, it fails to select one region with obvious high density\n",
    "- We want to be somehow conservative and only samples with a definitive association so better to go with an intermediate value like 10 or 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM = ({\"min_cluster_size\": 3}, {\"min_cluster_size\": 5}, {\"min_cluster_size\": 7}, \n",
    "         {\"min_cluster_size\": 10}, {\"min_cluster_size\": 12}, {\"min_cluster_size\": 15}, \n",
    "         {\"min_cluster_size\": 20}, {\"min_cluster_size\": 25}, {\"min_cluster_size\": 30})\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12,18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, param in enumerate(PARAM):\n",
    "    hdb = HDBSCAN(algorithm=\"auto\", cluster_selection_method=\"eom\", \n",
    "                           **param).fit(vector_mat)\n",
    "    labels = hdb.labels_\n",
    "    \n",
    "    plot(vector_mat, labels, hdb.probabilities_, param, ax=axes[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check the importance of `min_samples` with minimum cluster sizes of >10 to define our final grouping approach according to what would be expected from the visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM = ({\"min_cluster_size\": 12, \"min_samples\": 3}, {\"min_cluster_size\": 12, \"min_samples\": 5},\n",
    "         {\"min_cluster_size\": 12, \"min_samples\": 7}, {\"min_cluster_size\": 12, \"min_samples\": 10},\n",
    "         {\"min_cluster_size\": 12, \"min_samples\": 12}, {\"min_cluster_size\": 12, \"min_samples\": 15},\n",
    "         {\"min_cluster_size\": 12, \"min_samples\": 20}, {\"min_cluster_size\": 12, \"min_samples\": 25},\n",
    "         {\"min_cluster_size\": 12, \"min_samples\": 30})\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12,18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, param in enumerate(PARAM):\n",
    "    hdb = HDBSCAN(algorithm=\"auto\", cluster_selection_method=\"eom\",\n",
    "                  **param).fit(vector_mat)\n",
    "    labels = hdb.labels_\n",
    "    \n",
    "    plot(vector_mat, labels, hdb.probabilities_, param, ax=axes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go with a `min_cluster_size=12` and `min_samples=7`. \n",
    "\n",
    "- They may not be absolutely defined but we do not want to lose a significant portion of our results\n",
    "- We use the implementation in the module `hdbscan` instead of the one in `sklearn-env` as the former \n",
    "  provides a tree-form topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb_final = hdbscan.HDBSCAN(algorithm=\"best\", cluster_selection_method=\"eom\",\n",
    "                            min_cluster_size=12, min_samples=7)\n",
    "\n",
    "hdb_final = hdb_final.fit(vector_mat)\n",
    "\n",
    "    # plot the selected clustering and the hierarchical tree\n",
    "fig, axs = plt.subplots(2, 1, figsize=(7, 12))\n",
    "plot(vector_mat, hdb_final.labels_, hdb_final.probabilities_, ax=axs[0])\n",
    "hdb_final.condensed_tree_.plot(select_clusters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new dataset with cluster assignment is created for subsampling. To allow visualization of the clustering hierarchy is better to use the library `hdbscan` for the algorithm instead of `sklearn`'s implementation\n",
    "\n",
    "- Non-clustered samples must be retained so they are extracted from the primary data frame\n",
    "- We do stratified sampling of the clustered samples, maintaining the ratio of representation in the primary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_mat['hdbscan'] = hdb_final.labels_\n",
    "vector_mat['cluster'] = vector_mat['hdbscan'].replace(-1, np.nan)\n",
    "\n",
    "    # stratified sampling\n",
    "strat_sample = (\n",
    "    vector_mat.groupby('cluster')[['t-SNE-1', 't-SNE-2', 'hdbscan', 'cluster']]\n",
    "    .apply(lambda x: x.sample(frac=0.7))\n",
    "    .droplevel(0) \n",
    "    )\n",
    "\n",
    "unclustered_df = vector_mat[vector_mat['hdbscan'] == -1] \n",
    "\n",
    "sample_df = pd.concat([unclustered_df, strat_sample], axis=0)\n",
    "sample_df\n",
    "print(sample_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final frequencies in sample\n",
    "\n",
    "print(\n",
    "    vector_mat['hdbscan']\n",
    "    .replace(-1, \"unclustered\")\n",
    "    .value_counts(normalize=True)\n",
    "    .reset_index(name=\"Frequency\")\n",
    "    .rename(columns={'hdbscan':'HDBSCAN cluster'})  )\n",
    "\n",
    "sample_df.to_csv('../../processed_data/clustered_subsample.txt', sep='\\t', header=False, columns=[])\n",
    "\n",
    "\n",
    "sour_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
